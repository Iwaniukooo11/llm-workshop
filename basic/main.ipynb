{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e8d046",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155268e",
   "metadata": {},
   "source": [
    "## distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f4db4",
   "metadata": {},
   "source": [
    "distilbert-base-uncased-finetuned-sst-2-english\n",
    "Binary sentiment (POSITIVE / NEGATIVE)\n",
    "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1b541",
   "metadata": {},
   "source": [
    "## SamLowe/roberta-base-go_emotions\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ecc4c",
   "metadata": {},
   "source": [
    "SamLowe/roberta-base-go_emotions\t\n",
    "28 fine-grained emotions (admiration → worry)\n",
    "https://huggingface.co/SamLowe/roberta-base-go_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90be84",
   "metadata": {},
   "source": [
    "## unitary/unbiased-toxic-roberta\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5e356",
   "metadata": {},
   "source": [
    "unitary/unbiased-toxic-roberta\t\n",
    "\n",
    "Toxicity & six sub-types (toxic, severe_toxic, obscene, etc.)\n",
    "\n",
    "\n",
    "https://huggingface.co/unitary/unbiased-toxic-roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d8a3b",
   "metadata": {},
   "source": [
    "## Hate-speech-CNERG/dehatebert-mono-english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8462d1",
   "metadata": {},
   "source": [
    "Hate-speech-CNERG/dehatebert-mono-english\n",
    "Hate / non-hate\n",
    "https://huggingface.co/Hate-speech-CNERG/dehatebert-mono-english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd3ed3",
   "metadata": {},
   "source": [
    "## classla/multilingual-IPTC-news-topic-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd7873",
   "metadata": {},
   "source": [
    "classla/multilingual-IPTC-news-topic-classifier\n",
    "\n",
    "205 IPTC NewsCodes topics (e.g., crime, culture, health)\n",
    "\n",
    "https://huggingface.co/classla/multilingual-IPTC-news-topic-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34f797",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5afd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers datasets evaluate bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e5443",
   "metadata": {},
   "source": [
    "Check that CUDA is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a37f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available in PyTorch: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c471f4",
   "metadata": {},
   "source": [
    "# Chosen model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e425dc6",
   "metadata": {},
   "source": [
    "distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10111dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "ds = load_dataset(\"go_emotions\", \"simplified\")\n",
    "\n",
    "tok_r = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "mdl_r = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"SamLowe/roberta-base-go_emotions\").to(\"cuda\")\n",
    "\n",
    "clf = pipeline(\"text-classification\",\n",
    "               model=mdl_r, tokenizer=tok_r,\n",
    "               device=0, batch_size=32, top_k=None)\n",
    "\n",
    "def predict_split(split):\n",
    "    out = clf(split[\"text\"])\n",
    "    return [{d[\"label\"]: d[\"score\"] for d in row} for row in out]\n",
    "\n",
    "valid_logits  = predict_split(ds[\"validation\"])\n",
    "test_logits   = predict_split(ds[\"test\"])\n",
    "ds[\"validation\"] = ds[\"validation\"].add_column(\"roberta_logits\", valid_logits)\n",
    "ds[\"test\"]       = ds[\"test\"].add_column(\"roberta_logits\",  test_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde150f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00871f97",
   "metadata": {},
   "source": [
    "# Creating the train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = ds[\"validation\"].features[\"labels\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emotion_probabilities_df(dataset_split):\n",
    "    texts = []\n",
    "    emotion_probs = {emotion: [] for emotion in emotion_labels}\n",
    "    \n",
    "    for text, roberta_logits in zip(dataset_split[\"text\"], dataset_split[\"roberta_logits\"]):\n",
    "        texts.append(text)\n",
    "        \n",
    "        for emotion in emotion_labels:\n",
    "            prob = roberta_logits.get(emotion, 0.0)\n",
    "            emotion_probs[emotion].append(prob)\n",
    "    \n",
    "    df_data = {'input_text': texts}\n",
    "    df_data.update(emotion_probs)\n",
    "    \n",
    "    df = pd.DataFrame(df_data)\n",
    "    return df\n",
    "\n",
    "def predict_with_dataset_optimized(dataset_split, batch_size=128):\n",
    "    texts = dataset_split[\"text\"]\n",
    "    temp_dataset = Dataset.from_dict({\"text\": texts})\n",
    "    \n",
    "    def predict_batch(batch):\n",
    "        try:\n",
    "            outputs = clf(batch[\"text\"], truncation=True, max_length=512)\n",
    "            predictions = [{d[\"label\"]: d[\"score\"] for d in row} for row in outputs]\n",
    "            return {\"roberta_logits\": predictions}\n",
    "        except Exception as e:\n",
    "            empty_preds = [{label: 0.0 for label in emotion_labels} for _ in range(len(batch[\"text\"]))]\n",
    "            return {\"roberta_logits\": empty_preds}\n",
    "    \n",
    "    result_dataset = temp_dataset.map(\n",
    "        predict_batch,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    predictions = result_dataset[\"roberta_logits\"]\n",
    "    return predictions\n",
    "\n",
    "def predict_with_dataset_progress(dataset_split, batch_size=128):\n",
    "    texts = dataset_split[\"text\"]\n",
    "    total_samples = len(texts)\n",
    "    chunk_size = 5000\n",
    "    all_predictions = []\n",
    "    \n",
    "    for start_idx in tqdm(range(0, total_samples, chunk_size)):\n",
    "        end_idx = min(start_idx + chunk_size, total_samples)\n",
    "        chunk_texts = texts[start_idx:end_idx]\n",
    "        \n",
    "        chunk_dataset = Dataset.from_dict({\"text\": chunk_texts})\n",
    "        \n",
    "        def predict_batch(batch):\n",
    "            outputs = clf(batch[\"text\"], truncation=True, max_length=512)\n",
    "            return {\"roberta_logits\": [{d[\"label\"]: d[\"score\"] for d in row} for row in outputs]}\n",
    "        \n",
    "        chunk_result = chunk_dataset.map(\n",
    "            predict_batch,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "        \n",
    "        all_predictions.extend(chunk_result[\"roberta_logits\"])\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "if 'roberta_logits' not in ds['train'].column_names:\n",
    "    try:\n",
    "        train_logits = predict_with_dataset_optimized(ds[\"train\"], batch_size=128)\n",
    "    except Exception as e:\n",
    "        train_logits = predict_with_dataset_progress(ds[\"train\"], batch_size=64)\n",
    "    \n",
    "    ds[\"train\"] = ds[\"train\"].add_column(\"roberta_logits\", train_logits)\n",
    "\n",
    "train_df = create_emotion_probabilities_df(ds[\"train\"])\n",
    "validation_df = create_emotion_probabilities_df(ds[\"validation\"])\n",
    "test_df = create_emotion_probabilities_df(ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_emotion_probabilities_df(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf21efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608de08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_cols = train_df.columns.drop(\"input_text\")\n",
    "emotion_values = train_df[emotion_cols].values\n",
    "emotion_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df15799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_3_indices = np.argpartition(emotion_values, -3, axis=1)[:, -3:]\n",
    "top_3_sorted = np.take_along_axis(top_3_indices,\n",
    "                                  np.argsort(np.take_along_axis(emotion_values, top_3_indices, axis=1), axis=1)[:, ::-1],\n",
    "                                  axis=1)\n",
    "train_df['emotion_1'] = [emotion_cols[idx] for idx in top_3_sorted[:, 0]]\n",
    "train_df['emotion_2'] = [emotion_cols[idx] for idx in top_3_sorted[:, 1]]\n",
    "train_df['emotion_3'] = [emotion_cols[idx] for idx in top_3_sorted[:, 2]]\n",
    "\n",
    "train_df['emotion_1_value'] = emotion_values[np.arange(len(emotion_values)), top_3_sorted[:, 0]]\n",
    "train_df['emotion_2_value'] = emotion_values[np.arange(len(emotion_values)), top_3_sorted[:, 1]]\n",
    "train_df['emotion_3_value'] = emotion_values[np.arange(len(emotion_values)), top_3_sorted[:, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d32ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d438cb4",
   "metadata": {},
   "source": [
    "## Clearly seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5dd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_clearly_seperated = train_df.copy()\n",
    "\n",
    "mask = (train_df_clearly_seperated['emotion_1_value'] > 0.6) & \\\n",
    "       (train_df_clearly_seperated['emotion_1_value'] >= 2 * train_df_clearly_seperated['emotion_2_value'])\n",
    "\n",
    "train_df_clearly_seperated = train_df_clearly_seperated[mask]\n",
    "\n",
    "print(f\"Original dataset size: {len(train_df)}\")\n",
    "print(f\"Filtered dataset size: {len(train_df_clearly_seperated)}\")\n",
    "print(f\"Percentage retained: {len(train_df_clearly_seperated) / len(train_df) * 100:.2f}%\")\n",
    "train_df_clearly_seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a679c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_df = train_df_clearly_seperated.copy()[[\"input_text\", \"emotion_1\"]].rename(columns={\"input_text\": \"input\", \"emotion_1\": \"target\"}).reset_index(drop=True)\n",
    "input_output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_df.to_csv(\"clearly_seperated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4fe4bc",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_df = train_df.copy()[[\"input_text\", \"emotion_1\"]].rename(columns={\"input_text\": \"input\", \"emotion_1\": \"target\"}).reset_index(drop=True)\n",
    "input_output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef329b0",
   "metadata": {},
   "source": [
    "# Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a338746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(input_text, prompt_template=None, max_new_tokens=10, do_sample=False, temperature=0.7):\n",
    "    if prompt_template is None:\n",
    "        prompt = f\"What single concept best explains the output? Input: {input_text}\"\n",
    "    else:\n",
    "        prompt = prompt_template.format(input_text=input_text)\n",
    "   \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "   \n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    if do_sample:\n",
    "        generation_kwargs[\"temperature\"] = temperature\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "   \n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7673e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_10 = input_output_df.head(10).copy()\n",
    "prompts = {\n",
    "    \"concept\": \"What single concept best explains the output? Input: {input_text}\",\n",
    "    \"emotion\": \"What emotion does this text convey? Text: {input_text}\\nEmotion:\",\n",
    "    \"topic\": \"Identify the main topic of this text in one word: {input_text}\\nTopic:\",\n",
    "    \"summary\": \"Summarize this in one word: {input_text}\\nWord:\"\n",
    "}\n",
    "\n",
    "for prompt_name, prompt_template in prompts.items():\n",
    "    first_10[f'pred_{prompt_name}'] = first_10['input'].apply(\n",
    "        lambda x: get_model_prediction(\n",
    "            x, \n",
    "            prompt_template=prompt_template,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=prompt_name != \"concept\",\n",
    "            temperature=0.1\n",
    "        )\n",
    "    )\n",
    "\n",
    "first_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_samples = input_output_df.groupby('target').head(10).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9920e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_prompt = \"What single concept best explains the output? Input: {input_text}\"\n",
    "\n",
    "grouped_samples['concept_prediction'] = grouped_samples['input'].apply(\n",
    "    lambda x: get_model_prediction(\n",
    "        x,\n",
    "        prompt_template=concept_prompt,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False\n",
    "    )\n",
    ")\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'prompt': grouped_samples['input'],\n",
    "    'output': grouped_samples['concept_prediction'],\n",
    "    'real_output': grouped_samples['target']\n",
    "})\n",
    "\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1784257",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_prompt = \"What single emotion best explains the what the author is feeling? Input: {input_text}\"\n",
    "\n",
    "grouped_samples['concept_prediction'] = grouped_samples['input'].apply(\n",
    "    lambda x: get_model_prediction(\n",
    "        x,\n",
    "        prompt_template=concept_prompt,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False\n",
    "    )\n",
    ")\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'prompt': grouped_samples['input'],\n",
    "    'output': grouped_samples['concept_prediction'],\n",
    "    'real_output': grouped_samples['target']\n",
    "})\n",
    "\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_samples = input_output_df.groupby('target').head(10).copy()\n",
    "\n",
    "concept_prompt = f\"What single emotion out of the following: {', '.join(emotion_cols)} best explains what the author is feeling? Input: {{input_text}}\"\n",
    "\n",
    "grouped_samples['concept_prediction'] = grouped_samples['input'].apply(\n",
    "    lambda x: get_model_prediction(\n",
    "        x,\n",
    "        prompt_template=concept_prompt,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False\n",
    "    )\n",
    ")\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'prompt': grouped_samples['input'],\n",
    "    'output': grouped_samples['concept_prediction'],\n",
    "    'real_output': grouped_samples['target']\n",
    "})\n",
    "\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64707939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(input_text):\n",
    "    prompt = f\"What single concept best explains the output? Input: {input_text}\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "first_10 = input_output_df.head(10).copy()\n",
    "first_10['model_prediction'] = first_10['input'].apply(get_model_prediction)\n",
    "first_10[['input', 'target', 'model_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_samples = input_output_df.groupby('target').head(10).copy()\n",
    "\n",
    "concept_prompt = f\"What single emotion out of the following: {', '.join(emotion_cols)} best explains what the author is feeling? Input: {{input_text}} Finish the sentence: Output:The author is feeling: \"\n",
    "\n",
    "grouped_samples['concept_prediction'] = grouped_samples['input'].apply(\n",
    "    lambda x: get_model_prediction(\n",
    "        x,\n",
    "        prompt_template=concept_prompt,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False\n",
    "    )\n",
    ")\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'prompt': grouped_samples['input'],\n",
    "    'output': grouped_samples['concept_prediction'],\n",
    "    'real_output': grouped_samples['target']\n",
    "})\n",
    "\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702e7a2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prediction(row):\n",
    "    prediction = row['output'].lower()\n",
    "    target = row['real_output'].lower()\n",
    "    if target in prediction:\n",
    "        return 1\n",
    "    \n",
    "    for emotion in emotion_cols:\n",
    "        if emotion.lower() in prediction and emotion.lower() != target:\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "result_df['prediction_true'] = result_df.apply(check_prediction, axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d017dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df[\"prediction_true\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_correct = (result_df[result_df[\"prediction_true\"] != 0][\"prediction_true\"]==1).sum()\n",
    "count_of_incorrect = (result_df[result_df[\"prediction_true\"] != 0][\"prediction_true\"]==-1).sum()\n",
    "total_count = result_df.shape[0]\n",
    "print(f\"Correct predictions: {count_of_correct} out of {total_count} which is {count_of_correct/total_count*100}%\")\n",
    "print(f\"Incorrect predictions: {count_of_incorrect} out of {total_count} which is {count_of_incorrect/total_count*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fd3f4",
   "metadata": {},
   "source": [
    "Dużo lepiej niż losowe :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa27975",
   "metadata": {},
   "source": [
    "# Input-output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bceb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model():\n",
    "    model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def get_llm_emotion_prediction(text, tokenizer, model, emotion_list):\n",
    "    emotions_str = \", \".join(emotion_list)\n",
    "    prompt = f\"What emotion from this list: [{emotions_str}] best describes this text: '{text}' Answer with just the emotion name:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    response = generated_text.strip().lower()\n",
    "    for emotion in emotion_list:\n",
    "        if emotion.lower() in response:\n",
    "            return emotion.lower()\n",
    "    words = response.split()\n",
    "    return words[0] if words else \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'input': ['Life is amazing'],\n",
    "    'target': ['happiness']\n",
    "})\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = list(emotion_cols)\n",
    "\n",
    "text = test_df['input'][0]\n",
    "bert_prediction = test_df['target'][0]\n",
    "\n",
    "print(f\"Input text: '{text}'\")\n",
    "print(f\"BERT prediction: '{bert_prediction}'\")\n",
    "\n",
    "llm_prediction = get_llm_emotion_prediction(text, tokenizer, model, emotion_list)\n",
    "\n",
    "print(f\"LLM prediction: '{llm_prediction}'\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = ['happiness', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'joy', 'love', 'excitement']\n",
    "text = test_df['input'][0]\n",
    "bert_prediction = test_df['target'][0]\n",
    "\n",
    "print(f\"Input text: '{text}'\")\n",
    "print(f\"BERT prediction: '{bert_prediction}'\")\n",
    "\n",
    "llm_prediction = get_llm_emotion_prediction(text, tokenizer, model, emotion_list)\n",
    "\n",
    "print(f\"LLM prediction: '{llm_prediction}'\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d42a7c",
   "metadata": {},
   "source": [
    "lepiej działa dla małej listy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f7d7e",
   "metadata": {},
   "source": [
    "# Using the implemented classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1dd3a",
   "metadata": {},
   "source": [
    "## HFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_model import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HGF = HuggingFaceModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42507cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HGF.predict(\"I'm happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "HGF.predict(\"I'm sad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d6f6d",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2066e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith('basic'):\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "else:\n",
    "    project_root = current_dir\n",
    "project_root\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa784433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\nextcloud\\Studia - PW\\semestr 6\\warsztaty_badawcze\\llm-workshop\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llm_model import LLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda95ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/opt-2.7b\n",
      "Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "# llm_model = LLMModel(model_name=\"microsoft/DialoGPT-large\")\n",
    "llm_model = LLMModel(model_name=\"facebook/opt-2.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd077d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 20\n",
      "Sample training data:\n",
      "  Input: 'Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead' -> Output: 'neutral'\n",
      "  Input: 'WHY THE FUCK IS BAYLESS ISOING' -> Output: 'anger'\n",
      "  Input: 'Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝' -> Output: 'gratitude'\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"clearly_seperated.csv\").rename(columns={\"target\":\"output\"}).head(20)\n",
    "x_train = train_data[\"input\"].tolist()\n",
    "y_train = train_data[\"output\"].tolist()\n",
    "training_pairs = [(x, y) for x, y in zip(x_train, y_train) if pd.notna(x) and pd.notna(y)]\n",
    "x_train_clean = [pair[0] for pair in training_pairs]\n",
    "y_train_clean = [pair[1] for pair in training_pairs]\n",
    "\n",
    "print(f\"Training examples: {len(x_train_clean)}\")\n",
    "print(f\"Sample training data:\")\n",
    "for i in range(min(3, len(x_train_clean))):\n",
    "    print(f\"  Input: '{x_train_clean[i]}' -> Output: '{y_train_clean[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c36823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept prediction prompt:\n",
      "In 2 words guess, what task is the model doing, the format is x_test -> y_test:\n",
      "Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead -> neutral\n",
      "WHY THE FUCK IS BAYLESS ISOING -> anger\n",
      "Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝 -> gratitude\n",
      "We need more boards and to create a bit more space for [NAME]. Then we’ll be good. -> desire\n",
      "It might be linked to the trust factor of your friend. -> neutral\n",
      "Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol  -> amusement\n",
      "Hello everyone. Im from Toronto as well. Can call and visit in personal if needed. -> neutral\n",
      "R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family. -> caring\n",
      "Thank you friend -> gratitude\n",
      "Fucking coward. -> anger\n",
      "that is what retardation looks like -> neutral\n",
      "Maybe that’s what happened to the great white at Houston zoo -> confusion\n",
      "You are going to do the dishes now -> neutral\n",
      "Slowing things down now -> neutral\n",
      "His name has already been released. Just can't post it here. -> neutral\n",
      "Stupidly stubborn / stubbornly stupid -> anger\n",
      "Mine was apparently [NAME] and the giant peach! -> neutral\n",
      "I miss them being alive -> sadness\n",
      "Super, thanks -> gratitude\n",
      "A new study just came out from China that it's actually too late. -> neutral\n",
      "What is this task?\n",
      "\n",
      "Predicted concept: �\n",
      "�\n",
      "�It is usually the same as a majorly.\n",
      "�And the truthfully, because the fact that is a million dollar billi-based on aproperformats, you can be sure to go on behalf of a new productively the top level of the mainframe of money-related to get this particularities that could not only to have a lot of course, then govizemolu. If so, with respect to me is still\n",
      "Training completed with 20 examples\n"
     ]
    }
   ],
   "source": [
    "llm_model.train(x_train_clean, y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714e5441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept prediction prompt:\n",
      "In 2 words guess, what task is the model doing, the format is x_test -> y_test:\n",
      "Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead -> neutral\n",
      "WHY THE FUCK IS BAYLESS ISOING -> anger\n",
      "Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝 -> gratitude\n",
      "We need more boards and to create a bit more space for [NAME]. Then we’ll be good. -> desire\n",
      "It might be linked to the trust factor of your friend. -> neutral\n",
      "Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol  -> amusement\n",
      "Hello everyone. Im from Toronto as well. Can call and visit in personal if needed. -> neutral\n",
      "R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family. -> caring\n",
      "Thank you friend -> gratitude\n",
      "Fucking coward. -> anger\n",
      "that is what retardation looks like -> neutral\n",
      "Maybe that’s what happened to the great white at Houston zoo -> confusion\n",
      "You are going to do the dishes now -> neutral\n",
      "Slowing things down now -> neutral\n",
      "His name has already been released. Just can't post it here. -> neutral\n",
      "Stupidly stubborn / stubbornly stupid -> anger\n",
      "Mine was apparently [NAME] and the giant peach! -> neutral\n",
      "I miss them being alive -> sadness\n",
      "Super, thanks -> gratitude\n",
      "A new study just came out from China that it's actually too late. -> neutral\n",
      "What is this task?\n",
      "\n",
      "Predicted concept: philosoph, as a little boy who will not too much more than anything that we are all the first of the most recent history of course, we have been made clear for the same as part in the only one of antonally, and achina, it is a single, my newspinning and it”\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'philosoph, as a little boy who will not too much more than anything that we are all the first of the most recent history of course, we have been made clear for the same as part in the only one of antonally, and achina, it is a single, my newspinning and it”'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.predict_concept(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9fc980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction prompt for 'I'm happy':\n",
      "You are a classificator\n",
      "Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead -> neutral\n",
      "WHY THE FUCK IS BAYLESS ISOING -> anger\n",
      "Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝 -> gratitude\n",
      "We need more boards and to create a bit more space for [NAME]. Then we’ll be good. -> desire\n",
      "It might be linked to the trust factor of your friend. -> neutral\n",
      "Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol  -> amusement\n",
      "Hello everyone. Im from Toronto as well. Can call and visit in personal if needed. -> neutral\n",
      "R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family. -> caring\n",
      "Thank you friend -> gratitude\n",
      "Fucking coward. -> anger\n",
      "that is what retardation looks like -> neutral\n",
      "Maybe that’s what happened to the great white at Houston zoo -> confusion\n",
      "You are going to do the dishes now -> neutral\n",
      "Slowing things down now -> neutral\n",
      "His name has already been released. Just can't post it here. -> neutral\n",
      "Stupidly stubborn / stubbornly stupid -> anger\n",
      "Mine was apparently [NAME] and the giant peach! -> neutral\n",
      "I miss them being alive -> sadness\n",
      "Super, thanks -> gratitude\n",
      "A new study just came out from China that it's actually too late. -> neutral\n",
      "Learn based on this.\n",
      "\n",
      "Now predict the output for: I'm happy\n",
      "Output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ACHEscape.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.predict(\"I'm happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0550859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction prompt for 'I'm sad':\n",
      "You are a classificator\n",
      "Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead -> neutral\n",
      "WHY THE FUCK IS BAYLESS ISOING -> anger\n",
      "Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝 -> gratitude\n",
      "We need more boards and to create a bit more space for [NAME]. Then we’ll be good. -> desire\n",
      "It might be linked to the trust factor of your friend. -> neutral\n",
      "Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol  -> amusement\n",
      "Hello everyone. Im from Toronto as well. Can call and visit in personal if needed. -> neutral\n",
      "R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family. -> caring\n",
      "Thank you friend -> gratitude\n",
      "Fucking coward. -> anger\n",
      "that is what retardation looks like -> neutral\n",
      "Maybe that’s what happened to the great white at Houston zoo -> confusion\n",
      "You are going to do the dishes now -> neutral\n",
      "Slowing things down now -> neutral\n",
      "His name has already been released. Just can't post it here. -> neutral\n",
      "Stupidly stubborn / stubbornly stupid -> anger\n",
      "Mine was apparently [NAME] and the giant peach! -> neutral\n",
      "I miss them being alive -> sadness\n",
      "Super, thanks -> gratitude\n",
      "A new study just came out from China that it's actually too late. -> neutral\n",
      "Learn based on this.\n",
      "\n",
      "Now predict the output for: I'm sad\n",
      "Output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['| This is the sunbathecatast. I※s, but that the same as a few months ago, you do not to make you can be a person who is a single-upwardenjoyable and ready to apropermitters are not quite honestly, the world‐toughness. This will not only when we are very much of the case. The majority of this is not long term for each daydreaming, then again, or not a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.predict(\"I'm sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ff53843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n",
       "      <td>gratitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>We need more boards and to create a bit more s...</td>\n",
       "      <td>desire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>It might be linked to the trust factor of your...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Aww... she'll probably come around eventually,...</td>\n",
       "      <td>amusement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Hello everyone. Im from Toronto as well. Can c...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>R/sleeptrain Might be time for some sleep trai...</td>\n",
       "      <td>caring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Thank you friend</td>\n",
       "      <td>gratitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Fucking coward.</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>that is what retardation looks like</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Maybe that’s what happened to the great white ...</td>\n",
       "      <td>confusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>You are going to do the dishes now</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Slowing things down now</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>His name has already been released. Just can't...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Stupidly stubborn / stubbornly stupid</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Mine was apparently [NAME] and the giant peach!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>I miss them being alive</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Super, thanks</td>\n",
       "      <td>gratitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>A new study just came out from China that it's...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              input     output\n",
       "0            0  Now if he does off himself, everyone will thin...    neutral\n",
       "1            1                     WHY THE FUCK IS BAYLESS ISOING      anger\n",
       "2            2  Yes I heard abt the f bombs! That has to be wh...  gratitude\n",
       "3            3  We need more boards and to create a bit more s...     desire\n",
       "4            4  It might be linked to the trust factor of your...    neutral\n",
       "5            5  Aww... she'll probably come around eventually,...  amusement\n",
       "6            6  Hello everyone. Im from Toronto as well. Can c...    neutral\n",
       "7            7  R/sleeptrain Might be time for some sleep trai...     caring\n",
       "8            8                                   Thank you friend  gratitude\n",
       "9            9                                    Fucking coward.      anger\n",
       "10          10                that is what retardation looks like    neutral\n",
       "11          11  Maybe that’s what happened to the great white ...  confusion\n",
       "12          12                 You are going to do the dishes now    neutral\n",
       "13          13                            Slowing things down now    neutral\n",
       "14          14  His name has already been released. Just can't...    neutral\n",
       "15          15              Stupidly stubborn / stubbornly stupid      anger\n",
       "16          16    Mine was apparently [NAME] and the giant peach!    neutral\n",
       "17          17                            I miss them being alive    sadness\n",
       "18          18                                      Super, thanks  gratitude\n",
       "19          19  A new study just came out from China that it's...    neutral"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data # musi miec input i output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffcac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = min(3, len(x_train_clean))\n",
    "x_test = x_train_clean[:test_size]\n",
    "y_test = y_train_clean[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5250965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept prediction prompt:\n",
      "In 2 words guess, what task is the model doing, the format is x_test -> y_test:\n",
      "Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead -> neutral\n",
      "WHY THE FUCK IS BAYLESS ISOING -> anger\n",
      "Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝 -> gratitude\n",
      "We need more boards and to create a bit more space for [NAME]. Then we’ll be good. -> desire\n",
      "It might be linked to the trust factor of your friend. -> neutral\n",
      "Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol  -> amusement\n",
      "Hello everyone. Im from Toronto as well. Can call and visit in personal if needed. -> neutral\n",
      "R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family. -> caring\n",
      "Thank you friend -> gratitude\n",
      "Fucking coward. -> anger\n",
      "that is what retardation looks like -> neutral\n",
      "Maybe that’s what happened to the great white at Houston zoo -> confusion\n",
      "You are going to do the dishes now -> neutral\n",
      "Slowing things down now -> neutral\n",
      "His name has already been released. Just can't post it here. -> neutral\n",
      "Stupidly stubborn / stubbornly stupid -> anger\n",
      "Mine was apparently [NAME] and the giant peach! -> neutral\n",
      "I miss them being alive -> sadness\n",
      "Super, thanks -> gratitude\n",
      "A new study just came out from China that it's actually too late. -> neutral\n",
      "What is this task?\n",
      "\n",
      "Predicted concept: ­ the truthfully for a few years ago.\n",
      "�tentreappearn”\n",
      "�s, but it seems to pay attention to know how many, even though, and at a very similar to keep asking people who cares, it is not to see more than the purposefulfied by the best thing at the first time, as anfans from the same as it was the fact that we are the next day, we have been in this article, such as\n",
      "Detected task: ­ the truthfully for a few years ago.\n",
      "�tentreappearn”\n",
      "�s, but it seems to pay attention to know how many, even though, and at a very similar to keep asking people who cares, it is not to see more than the purposefulfied by the best thing at the first time, as anfans from the same as it was the fact that we are the next day, we have been in this article, such as\n"
     ]
    }
   ],
   "source": [
    "concept = llm_model.predict_concept(x_train, y_train)\n",
    "print(f\"Detected task: {concept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_model import LLMModel\n",
    "LLM_model = LLMModel()\n",
    "LLM_model.demo_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(x_train_clean) > 3:\n",
    "    new_inputs = x_train_clean[-2:]  # Last 2 examples as test\n",
    "    predictions = llm_model.predict(new_inputs)\n",
    "    \n",
    "    print(\"Prediction results:\")\n",
    "    for inp, pred in zip(new_inputs, predictions):\n",
    "        print(f\"  Input: '{inp}' -> Prediction: '{pred}'\")\n",
    "else:\n",
    "    print(\"Not enough data for separate prediction examples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
